Lexical analyzer
input:
    - token.in - all reserved words, operators, and separators;
    - p*.in - code to be analyzed;
output:
    - stdout - lexical errors or lack thereof;
    - PIFfile.out - program internal form, if execution succeeds;
    - ST.out - symbol table, if execution succeeds;

tokenizes code input, then for each token:
    - if reserved word, operator, or separator, adds entry to pif;
    - if valid identifier or constant, adds entries to sl and pif;
    - otherwise, prints error;
if no errors occur, write sl and pif in output files;

regex:
    - one regex pattern for identifiers, strings, and numbers
    - one regex that matches all separators, which we use to tokenize
